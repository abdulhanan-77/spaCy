{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Word Vectors and spaCy"
      ],
      "metadata": {
        "id": "jx86O9cBAAEi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcnrQIe1_zDv",
        "outputId": "5987ce06-de65-4c42-ce5b-9bab67e2b532"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-md==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2024.7.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_md"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "JSn8b0xVAj8X"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "with open (\"/content/wiki_us.txt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "doc = nlp(text)\n",
        "sentence = list(doc.sents)[0]\n",
        "sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OBFG-tpAeXY",
        "outputId": "f81f1eb6-45f7-4e01-8e41-0352c2c84398"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "The United States of America (U.S.A. or USA), commonly known as the United States (U.S. or US) or America, is a country primarily located in North America."
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Word Vectors\n",
        "Word vectors, or word embeddings, are numerical representations of words in multidimensional space through matrices. The purpose of the word vector is to get a computer system to understand a word. Computers cannot understand text efficiently. They can, however, process numbers quickly and well. For this reason, it is important to convert a word into a number.\n",
        "\n",
        "Initial methods for creating word vectors in a pipeline take all words in a corpus and convert them into a single, unique number. These words are then stored in a dictionary that would look like this: {“the”: 1, “a”, 2} etc. This is known as a bag of words. This approach to representing words numerically, however, only allow a computer to understand words numerically to identify unique words. It does not, however, allow a computer to understand meaning.\n",
        "\n",
        "Imagine this scenario:\n",
        "\n",
        "Tom loves to eat chocolate.\n",
        "\n",
        "Tom likes to eat chocolate.\n",
        "\n",
        "These sentences represented as a numerical array (list) would look like this:\n",
        "\n",
        "1, 2, 3, 4, 5\n",
        "\n",
        "1, 6, 3, 4, 5\n",
        "\n",
        "As we can see, as humans both sentences are nearly identical. The only difference is the degree to which Tom appreciates eating chocolate. If we examine the numbers, however, these two sentences seem quite close, but their semantical meaning is impossible to know for certain. How similar is 2 to 6? The number 6 could represent “hates” as much as it could represent “likes”. This is where word vectors come in.\n",
        "\n",
        "Word vectors take these one dimensional bag of words and gives them multidimensional meaning by representing them in higher dimensional space, noted above. This is achieved through machine learning and can be easily achieved via Python libraries, such as Gensim, which we will explore more closely in the next notebook."
      ],
      "metadata": {
        "id": "pigcrylpAV8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.1. Why use word vectors?\n",
        "\n",
        "The goal of word vectors is to achieve numerical understanding of language so that a computer can perform more complex tasks on that corpus. Let’s consider the example above. How do we get a computer to understand 2 and 6 are synonyms or mean something similar? One option you might be thinking is to simply give the computer a synonym dictionary. It can look up synonyms and then know what words mean. This approach, on the surface, makes perfect sense, but let’s explore that option and see why it cannot possibly work.\n",
        "\n",
        "For the example below, we will be using the Python library PyDictionary which allows us to look up definitions and synonyms of words."
      ],
      "metadata": {
        "id": "xBmZoGZQAylX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydictionary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeGTOItbA_sE",
        "outputId": "f6defe06-93ba-4e5f-8020-7adf516abb7a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydictionary in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.10/dist-packages (from pydictionary) (0.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from pydictionary) (8.1.7)\n",
            "Requirement already satisfied: goslate in /usr/local/lib/python3.10/dist-packages (from pydictionary) (1.5.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pydictionary) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4->pydictionary) (4.12.3)\n",
            "Requirement already satisfied: futures in /usr/local/lib/python3.10/dist-packages (from goslate->pydictionary) (3.0.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pydictionary) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pydictionary) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pydictionary) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pydictionary) (2024.7.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4->pydictionary) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PyDictionary import PyDictionary\n",
        "\n",
        "dictionary = PyDictionary()\n",
        "text = \"Tom loves to eat chocolate\"\n",
        "\n",
        "words = text.split()\n",
        "for word in words:\n",
        "  syns = dictionary.synonym(word)\n",
        "  print(f\"{word}: {syns[0:5]}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "fnwvOellBANC",
        "outputId": "28fc5829-4a0e-4405-c239-ea08481e11fa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tom has no Synonyms in the API\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'NoneType' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-327e9333fdf5>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0msyns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynonym\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{word}: {syns[0:5]}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.2. What do Word Vectors Look Like?\n",
        "Word vectors have a preset number of dimensions. These dimensions are honed via machine learned. Models take into account word frequency alongside words across a corpus and the appearance of other words in similar contexts. This allows for the the computer to determine the syntactical similarity of words numerically. It then needs to represent these relationships numerically. It does this through the vector, or a matrix of matrices. To represent these more concisely, models flatten a matrix to a float (decimal number). The number of dimensions represent the number of floats in the matrix.\n",
        "\n",
        "Let’s take a look at the first word in our sentence. Specifically, let’s look at its vector."
      ],
      "metadata": {
        "id": "sNSnYfkvBJsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence[0].vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqH6epktBPbl",
        "outputId": "32dca60d-3029-4e0d-bc6a-598378a00cea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-7.2681e+00, -8.5717e-01,  5.8105e+00,  1.9771e+00,  8.8147e+00,\n",
              "       -5.8579e+00,  3.7143e+00,  3.5850e+00,  4.7987e+00, -4.4251e+00,\n",
              "        1.7461e+00, -3.7296e+00, -5.1407e+00, -1.0792e+00, -2.5555e+00,\n",
              "        3.0755e+00,  5.0141e+00,  5.8525e+00,  7.3378e+00, -2.7689e+00,\n",
              "       -5.1641e+00, -1.9879e+00,  2.9782e+00,  2.1024e+00,  4.4306e+00,\n",
              "        8.4355e-01, -6.8742e+00, -4.2949e+00, -1.7294e-01,  3.6074e+00,\n",
              "        8.4379e-01,  3.3419e-01, -4.8147e+00,  3.5683e-02, -1.3721e+01,\n",
              "       -4.6528e+00, -1.4021e+00,  4.8342e-01,  1.2549e+00, -4.0644e+00,\n",
              "        3.3278e+00, -2.1590e-01, -5.1786e+00,  3.5360e+00, -3.1575e+00,\n",
              "       -3.5273e+00, -3.6753e+00,  1.5863e+00, -8.1594e+00, -3.4657e+00,\n",
              "        1.5262e+00,  4.8135e+00, -3.8428e+00, -3.9082e+00,  6.7549e-01,\n",
              "       -3.5787e-01, -1.7806e+00,  3.5284e+00, -5.1114e-02, -9.7150e-01,\n",
              "       -9.0553e-01, -1.5570e+00,  1.2038e+00,  4.7708e+00,  9.8561e-01,\n",
              "       -2.3186e+00, -7.4899e+00, -9.5389e+00,  8.5572e+00,  2.7420e+00,\n",
              "       -3.6270e+00,  2.7456e+00, -6.9574e+00, -1.7190e+00, -2.9145e+00,\n",
              "        1.1838e+00,  3.7864e+00,  2.0413e+00, -3.5808e+00,  1.4319e+00,\n",
              "        2.0528e-01, -7.0640e-01, -5.3556e+00, -2.5911e+00,  4.4922e+00,\n",
              "        1.6574e+00,  3.9794e+00, -4.3560e+00, -2.7266e+00,  1.9581e+00,\n",
              "       -3.4842e+00, -3.9674e+00,  3.2690e+00,  6.6683e-01,  3.9837e+00,\n",
              "       -6.5997e+00,  4.1630e+00,  8.0338e+00,  3.8102e-01,  8.2656e+00,\n",
              "        9.7061e-01, -5.0807e+00,  4.9522e+00,  7.5018e+00,  3.8305e+00,\n",
              "       -3.3233e+00,  4.9126e+00,  2.4189e-01,  3.8218e+00, -3.9717e+00,\n",
              "        2.4691e+00,  1.3721e+01, -8.9664e+00,  1.0610e+01,  6.9425e-01,\n",
              "       -1.1082e+01, -5.6883e+00,  2.3287e+00,  1.6451e+00,  3.6006e+00,\n",
              "        1.2588e-01, -6.1956e+00,  1.1455e+01,  5.6682e+00, -5.0251e-01,\n",
              "       -9.8515e-01,  8.8902e-02, -4.0213e+00,  3.6134e+00, -9.0936e+00,\n",
              "       -1.4555e+01, -2.5591e+00,  4.0959e+00, -3.5929e-01,  1.0219e+00,\n",
              "        3.9402e+00,  8.0495e-01, -3.6023e+00,  2.6394e+00, -1.5258e-01,\n",
              "       -2.6182e+00, -2.6268e-01, -2.1610e+00,  2.3950e+00,  6.8842e+00,\n",
              "        3.6034e+00,  1.8058e+00,  2.4528e+00,  4.4088e+00, -1.0598e+00,\n",
              "        6.4964e+00,  5.9196e+00, -1.0261e+00, -1.7013e+00, -4.4151e+00,\n",
              "        4.3043e+00, -1.7138e+00, -4.6690e+00, -5.5212e-01,  5.3995e+00,\n",
              "        1.8311e+00, -3.5820e-01, -3.6578e-01, -2.8578e+00, -6.4639e+00,\n",
              "       -3.2155e+00,  6.7083e-01, -1.2800e+00,  1.2782e+00,  7.8274e-01,\n",
              "        1.9839e-01, -1.4163e+00,  2.1184e+00,  1.5021e+00, -1.8212e+00,\n",
              "        1.6629e+00,  4.0354e+00, -4.4648e+00, -3.4897e+00, -2.5765e+00,\n",
              "       -3.6317e+00, -4.1619e-02,  4.8660e-01,  2.0712e+00, -1.9166e+00,\n",
              "       -3.4045e+00, -7.6609e+00, -2.1940e+00, -2.3919e-03,  8.4900e-01,\n",
              "        1.3921e+00, -5.7830e+00,  4.4739e+00,  1.0642e+00,  5.7864e+00,\n",
              "        3.4643e+00, -5.9169e+00, -2.6925e+00, -1.1271e-01, -6.0462e+00,\n",
              "        3.9285e+00, -3.0423e+00, -6.9939e-02,  2.2826e-01,  8.0214e+00,\n",
              "        2.2098e+00, -1.1049e+01,  7.6001e-02, -1.5970e+00,  2.0524e-01,\n",
              "        2.8063e+00,  3.5245e+00, -3.9300e+00, -9.7995e-01,  4.0248e+00,\n",
              "        1.8447e+00, -2.0452e+00,  1.1419e+00, -4.4600e-01, -9.5551e-01,\n",
              "       -1.0224e+00,  5.9224e+00, -6.1688e+00, -8.3840e-01, -7.9102e+00,\n",
              "       -8.9575e-02, -2.7741e-01,  4.2703e+00,  4.0212e+00, -1.1166e-01,\n",
              "        2.5119e+00, -5.9635e+00, -1.2320e+00,  2.8199e-01, -4.1062e+00,\n",
              "       -6.2923e-01, -5.2420e-01,  2.5213e+00, -3.5094e+00,  6.4333e+00,\n",
              "        7.9466e+00, -3.3883e+00,  5.2535e+00,  9.4524e-02, -3.3336e+00,\n",
              "        5.9621e+00, -1.0794e+00, -6.0850e+00, -3.6071e+00, -3.8496e-01,\n",
              "        7.6137e+00, -9.1081e+00, -6.0037e+00, -2.4735e+00, -6.5050e-01,\n",
              "       -6.3021e+00,  8.5783e+00,  1.7250e-01,  4.3631e+00, -9.3439e+00,\n",
              "        2.0984e-01,  7.6900e-01,  1.0763e+01,  4.4598e-01, -3.6584e+00,\n",
              "       -3.0992e+00, -3.8868e+00,  4.3337e+00, -5.8037e+00, -1.1337e+00,\n",
              "       -6.1562e+00,  3.1820e-01, -1.0612e+00, -1.4809e+00,  6.0373e+00,\n",
              "        4.6015e-01, -1.5530e+00, -1.0562e+00,  5.8618e-01,  3.4431e+00,\n",
              "        4.5542e+00, -3.1881e+00, -1.5832e+00,  3.0859e+00,  1.3061e+00,\n",
              "       -8.0091e+00,  7.7996e+00, -5.0644e+00,  8.8719e+00,  7.2337e-01,\n",
              "       -1.2350e+00,  1.6209e+00,  7.8994e+00,  1.0741e+01,  8.1158e-01,\n",
              "        9.0156e+00, -1.5913e+00, -5.3166e+00,  3.5032e-01, -2.8850e+00],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.3. Why use Word Vectors?\n",
        "\n",
        "Once a word vector model is trained, we can do similarity matches very quickly and very reliably.\n",
        "\n"
      ],
      "metadata": {
        "id": "X7lO9-QGBYnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "your_word = \"country\"\n",
        "\n",
        "ms = nlp.vocab.vectors.most_similar(\n",
        "    np.asarray([nlp.vocab.vectors[nlp.vocab.strings[your_word]]]), n=10)\n",
        "\n",
        "words = [nlp.vocab.strings[w] for w in ms[0][0]]\n",
        "distances = ms[2]\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUsgSTYsBUAu",
        "outputId": "1a006a7e-4671-4cb5-84e1-9ed49cadfc24"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['country—0,467', 'nationâ\\x80\\x99s', 'countries-', 'continente', 'Carnations', 'pastille', 'бесплатно', 'Argents', 'Tywysogion', 'Teeters']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. Doc Similarity\n",
        "\n",
        "In spaCy we can do this same thing at the document level. Through word vectors we can calculate the similarity between two documents."
      ],
      "metadata": {
        "id": "jhXQeizxC0vM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc1 = nlp(\"I like salty fries at hamburgers.\")\n",
        "doc2 = nlp(\"Fast food tastes very good.\")"
      ],
      "metadata": {
        "id": "qHUE5f_yBhA-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc1, \"<->\", doc2, doc1.similarity(doc2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2TKd1HrB6xW",
        "outputId": "7cce6f2b-c122-4cd5-dbe8-264900fbbe43"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I like salty fries at hamburgers. <-> Fast food tastes very good. 0.5812673238985657\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc3 = nlp(\"The Empire State Building is in New York.\")"
      ],
      "metadata": {
        "id": "83fta1oMB8qe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc1, \"<->\", doc3, doc1.similarity(doc3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ha5qxp9yB_LW",
        "outputId": "2a15029b-83d7-4d37-d029-2fabd8c5775b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I like salty fries at hamburgers. <-> The Empire State Building is in New York. 0.16940421517595508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc4 = nlp(\"I enjoy oranges.\")\n",
        "doc5 = nlp(\"I enjoy apples.\")"
      ],
      "metadata": {
        "id": "1YPA7NI0CDAi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc4, \"<->\", doc5, doc4.similarity(doc5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQqoVb3tCGKi",
        "outputId": "1b5a61a9-09d2-4a5f-e0ba-e1060c1aaada"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I enjoy oranges. <-> I enjoy apples. 0.9775702131220241\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc6 = nlp(\"I enjoy fries.\")"
      ],
      "metadata": {
        "id": "qTeWzqbKCI6K"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc4, \"<->\", doc6, doc4.similarity(doc6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCZFRzBXCLCT",
        "outputId": "a0887f1a-5ecb-4245-83d5-022d5e314872"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I enjoy oranges. <-> I enjoy fries. 0.9592555441896311\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3. Word Similarity\n",
        "\n",
        "We can also calculate the similarity between two given words."
      ],
      "metadata": {
        "id": "DB-bpt3PDFTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "french_fries = doc1[2:4]\n",
        "burgers = doc1[5]\n",
        "print(french_fries, \"<->\", burgers, french_fries.similarity(burgers))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVuZ1YYnCOQS",
        "outputId": "2d8aa9bc-8962-4731-cef5-80330e9ca5de"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "salty fries <-> hamburgers 0.6938489675521851\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\")"
      ],
      "metadata": {
        "id": "FLLz-cKyDorP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.add_pipe(\"sentencizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kz5R7ByNJaFQ",
        "outputId": "3843db60-655b-458d-8237-483baebd826c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.pipeline.sentencizer.Sentencizer at 0x78e7272ef680>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SpaCy's Pipelines\n",
        "\n"
      ],
      "metadata": {
        "id": "5sQLxnuhK3an"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will be learning about the various pipelines in spaCy. As we have seen, spaCy offers both heuristic (rules-based) and machine learning natural language processing solutions. These solutions are activated by pipes."
      ],
      "metadata": {
        "id": "pZJEELMqLETS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1. Standard Pipes (Components and Factories) Available from spaCy\n",
        "\n",
        "SpaCy is much more than an NLP framework. It is also a way of designing and implementing complex pipelines. A pipeline is a sequence of pipes, or actors on data, that make alterations to the data or extract information from it. In some cases, later pipes require the output from earlier pipes. In other cases, a pipe can exist entirely on its own.\n",
        "\n"
      ],
      "metadata": {
        "id": "LhcrkBiQLGI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.1. Attribute Rulers\n",
        "\n",
        "* Dependency Parser\n",
        "\n",
        "* EntityLinker\n",
        "\n",
        "* EntityRecognizer\n",
        "\n",
        "* EntityRuler\n",
        "\n",
        "* Lemmatizer\n",
        "\n",
        "* Morpholog\n",
        "\n",
        "* SentenceRecognizer\n",
        "\n",
        "* Sentencizer\n",
        "\n",
        "* SpanCategorizer\n",
        "\n",
        "* Tagger\n",
        "\n",
        "* TextCategorizer\n",
        "\n",
        "* Tok2Vec\n",
        "\n",
        "* Tokenizer\n",
        "\n",
        "* TrainablePipe\n",
        "\n",
        "* Transformer"
      ],
      "metadata": {
        "id": "jk9BtiZgOOLk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.2. Matchers\n",
        "\n",
        "* DependencyMatcher\n",
        "\n",
        "* Matcher\n",
        "\n",
        "* PhraseMatcher"
      ],
      "metadata": {
        "id": "BEk3BiGucRS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2. How to Add Pipes\n",
        "\n",
        "In most cases, you will use an off-the-shelf spaCy model. In some cases, however, an off-the-shelf model will not fill your needs or will perform a specific task very slowly. A good example of this is sentence tokenization. Imagine if you had a document that was around 1 million sentences long. Even if you used the small English model, your model would take a long time to process those 1 million sentences and separate them. In this instance, you would want to make a blank English model and simply add the Sentencizer to it. The reason is because each pipe in a pipeline will be activated (unless specified) and that means that each pipe from Dependency Parser to named entity recognition will be performed on your data. This is a serious waste of computational resources and time. The small model may take hours to achieve this task. By creating a blank model and simply adding a Sentencizer to it, you can reduce this time to merely minutes."
      ],
      "metadata": {
        "id": "HLAugBH6cZbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\")"
      ],
      "metadata": {
        "id": "f50cy5-EJc2T"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, notice that we have used spacy.blank, rather than spacy.load. When we create a blank model, we simply pass the two letter combination for a language, in this case, en for English. Now, let’s use the add_pipe() command to add a new pipe to it. We will simply add a sentencizer."
      ],
      "metadata": {
        "id": "B54bkSHOdKpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.add_pipe(\"sentencizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NvcMwvndFpA",
        "outputId": "ffcfb16f-3f11-4c39-fe04-3e85ea01ac2c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.pipeline.sentencizer.Sentencizer at 0x78e6454fb340>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "s = requests.get(\"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\")\n",
        "soup = BeautifulSoup(s.content).text.replace(\"-\\n\", \"\").replace(\"\\n\", \" \")\n",
        "\n",
        "nlp.max_length = 5278439"
      ],
      "metadata": {
        "id": "JCKBK5bNdR55"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "doc = nlp(soup)\n",
        "print(len(list(doc.sents)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCBUtgI8d3_I",
        "outputId": "8c606531-12e6-4b43-d07a-7880b5df905d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94134\n",
            "CPU times: user 15.6 s, sys: 405 ms, total: 16 s\n",
            "Wall time: 16.1 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3. Examining a Pipeline\n",
        "\n",
        "In spaCy, we have a few different ways to study a pipeline. If we want to do this in a script, we can do the following command:"
      ],
      "metadata": {
        "id": "bmUKmdUtfEoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.analyze_pipes()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQw38vAcd21z",
        "outputId": "b86e9884-7285-478b-a38d-bd82752b36e3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': {'sentencizer': {'assigns': ['token.is_sent_start', 'doc.sents'],\n",
              "   'requires': [],\n",
              "   'scores': ['sents_f', 'sents_p', 'sents_r'],\n",
              "   'retokenizes': False}},\n",
              " 'problems': {'sentencizer': []},\n",
              " 'attrs': {'token.is_sent_start': {'assigns': ['sentencizer'], 'requires': []},\n",
              "  'doc.sents': {'assigns': ['sentencizer'], 'requires': []}}}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note the dictionary structure. This tells us not only what is inside the pipeline, but its order. Each key after “summary” is a pipe. The value is a dictionary. This dictionary tells us a few different things. All of these value dictionaries state: “assigns” which corresponds to a value of what that particular pipe assigns to the token and doc as it passes through the pipeline. In some cases, there will be a key of “scores” in the dictionary. This indicates how the machine learning model was evaluated. We will learn more about model evaluation in our machine learning section below."
      ],
      "metadata": {
        "id": "5Ycp4VDMftG3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ACsBswOjfRuQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}